{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a235030c",
   "metadata": {},
   "source": [
    "### 08/06/25: This is the start of our initial analysis, anything we do will be added here from time to time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f890733-c4b8-4375-83ad-37b8774f0b0a",
   "metadata": {},
   "source": [
    "### Our rough project outline:(Each steps can be broken down to several more sub steps to finish a working project)\n",
    "    1. Find and gather proper datasets and a model to train\n",
    "    2. Clean it if needed.\n",
    "    3. Train relavant model with our ready dataset.\n",
    "    4. Try to see to implement and test our trained the dataset with similar existing dataset to see if the training to close to reality.\n",
    "    5. Train it in multiple iteration and record the progress/decline in performance and predictibility.\n",
    "    6. Document the performance shift after each training iteration\n",
    "    7. Check if the outcome changes according to Genre or other classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be81516-cd6c-4eaa-b09a-6511f851d42a",
   "metadata": {},
   "source": [
    "### Minimum Viable Product(MVP)\n",
    "    1. Focus solely on rating prediction based on Book title and description(Priority feature)\n",
    "    2. First output head we focus on is the rating prediction head as our main purpose of the model is to predict if it can be successfully predict how successful it can be with the current description and title.\n",
    "    3. Secondary task will be to genre classification, if we have time. \n",
    "\n",
    "### Plans to achieve it:\n",
    "    1. Data Cleaning: \n",
    "        1.1. In order to ensure no duplicate data is present, first sort all the cleanly categorized(title, description, genre, rating, author) dataset together into one single file and search for duplicates to make sure if any data is not repeated.\n",
    "        1.2. Make sure the null value rows are deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce6a544-49a9-4fa6-82ac-611aadeb2ee4",
   "metadata": {},
   "source": [
    "## Inspired from this previous idea\n",
    "\n",
    "    Group 1: Original Book-Focused Projects \n",
    "    Multi-Task Book Title Analyzer and Generator \n",
    "    \n",
    "    Category: NLP, Multi-Task Learning, Text Generation\n",
    "    \n",
    "    Idea: Create a unified system that performs title rating prediction, genre classification, and optimized title generation simultaneously. \n",
    "    \n",
    "    ## Implementation: \n",
    "    \n",
    "    Use a shared encoder architecture (BERT or ROBERTa) to capture common features across tasks. \n",
    "    \n",
    "    Implement separate output heads for each task: \n",
    "    \n",
    "    Rating Prediction (Regression): A regression layer that predicts a continuous rating score. \n",
    "\n",
    "    Genre Classification: A classification layer that predicts the book genre from a \n",
    "    predefined set of categories. \n",
    "    \n",
    "    Title Generation: A decoder network (T5 or BART) generates optimized titles \n",
    "    based on the encoded features. \n",
    "    \n",
    "    Train the system jointly with a multi-task loss function, combining the losses from each task. \n",
    "    \n",
    "    Datasets: Goodreads dataset, Amazon Book Reviews \n",
    "    \n",
    "    Tools: Hugging Face Transformers, PyTorch, Sentence-BERT \n",
    "    \n",
    "    GPU Requirements: No local GPU required for development and small-scale experiments. Google Colab or Kaggle provide free GPU access for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc63afbc-5cd3-4139-88a1-7d843df05dc2",
   "metadata": {},
   "source": [
    "# After combining our rough outline and the previous outline, this is what we came up with after organizing:\n",
    "\n",
    "## **Category:**\n",
    "\n",
    "Natural Language Processing (NLP), Multi-Task Learning, Text Generation\n",
    "\n",
    "---\n",
    "\n",
    "## **Project Objective:**\n",
    "\n",
    "Build a unified system that performs **three book-related NLP tasks simultaneously**:\n",
    "\n",
    "* **Book Rating Prediction** (Regression)\n",
    "* **Genre Classification** (Multi-label Classification)\n",
    "* **Title Generation** (Text Generation)\n",
    "\n",
    "---\n",
    "\n",
    "## **High-Level Workflow (Your Outline Enhanced):**\n",
    "\n",
    "### **Step 1: Find and Gather Proper Datasets and Pretrained Models**\n",
    "\n",
    "* Use sources like **Goodreads**, **Amazon Book Reviews**, or **open Kaggle datasets** that contain book titles, user ratings, genres, and possibly book summaries or reviews.\n",
    "* Select suitable base models:\n",
    "\n",
    "  * **Shared Encoder:** BERT, RoBERTa, or Sentence-BERT\n",
    "  * **Text Generator/Decoder:** T5 or BART for generating titles\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Clean and Preprocess the Data**\n",
    "\n",
    "* Remove duplicates, nulls, or corrupted entries\n",
    "* Normalize genre labels (unify formats, handle multi-genre books)\n",
    "* Tokenize titles and summaries for model input\n",
    "* Possibly apply text cleaning (e.g., removing HTML tags, emojis, special characters)\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Build and Train the Multi-Task Model**\n",
    "\n",
    "* Use a **shared encoder** to extract common features.\n",
    "* Design three separate output heads:\n",
    "\n",
    "  * **Rating Prediction Head:** Linear regression layer\n",
    "  * **Genre Classification Head:** Fully connected layer with softmax or sigmoid (multi-label)\n",
    "  * **Title Generation Head:** Decoder network (T5/BART)\n",
    "* Use a **multi-task loss function** that combines:\n",
    "\n",
    "  * Mean Squared Error for rating\n",
    "  * Cross-entropy or binary cross-entropy for genre\n",
    "  * Sequence loss (e.g., negative log-likelihood) for title generation\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Test on Similar Existing Datasets**\n",
    "\n",
    "* Validate the model on other datasets with similar structures\n",
    "* Check generalization to different genres, styles, or ratings distributions\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 5: Train in Multiple Iterations and Track Performance**\n",
    "\n",
    "* Train the model in **phases or epochs**, saving outputs after each iteration\n",
    "* Experiment with different hyperparameters and loss weightings between tasks\n",
    "* Use early stopping, learning rate schedulers, and model checkpoints\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 6: Document Performance Changes**\n",
    "\n",
    "* Log training metrics after each iteration (loss, accuracy, BLEU score for generation)\n",
    "* Visualize progress using graphs (loss curves, F1-score, etc.)\n",
    "* Record qualitative examples of generated titles over time\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 7: Analyze Outcome by Genre and Other Attributes**\n",
    "\n",
    "* Compare model performance across different genres:\n",
    "\n",
    "  * Are romance titles easier to predict than sci-fi?\n",
    "  * Does the generated title style change across genres?\n",
    "* Correlate rating prediction accuracy with genre, length of the title, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## **Tools & Technologies**\n",
    "\n",
    "* **Frameworks**: PyTorch, Hugging Face Transformers\n",
    "* **Libraries**: Scikit-learn, Pandas, Matplotlib, Seaborn\n",
    "* **Training Environment**: Google Colab or Kaggle (free GPU access)\n",
    "* **Optional**: Streamlit or Gradio for interactive demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "443bb676-cd81-4080-bf20-10672cb65ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SafranBro.com\n"
     ]
    }
   ],
   "source": [
    "print(\"SafranBro.com\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
